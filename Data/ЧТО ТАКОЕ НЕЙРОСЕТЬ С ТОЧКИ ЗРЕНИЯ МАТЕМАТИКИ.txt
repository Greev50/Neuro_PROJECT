С точки зрения математики, нейросеть оказывается куда сложнее в понимании, нежели в обыденном объяснении.
Давайте я приведу наглядный пример. На нем будет куда проще объяснить.

Сейчас мы с вами будем проектировать нейросеть, которая сможет распознавать все рукописные цифры на картинке 28 на 28 пикселей. 
Давайте представим нейрон. Нейрон - просто некая ячейка, содержащая в себе некое число от 0 до 1. 
Само число в нем называется активацией. Активация - значение, которое отражает, насколько сильно нейрон активируется в ответ на входные данные. Если активация больше какого то определенного порога, то нейрон зажигается. Все достаточно просто. 

В первом слое нейросети находятся нейроны, которые как раз получают входные данные и передают их далее. В нашем случае у нас картинка 28*28 пикселей, то есть на входе стоит 784 нейрона.
Во входных нейронах активация выражает градацию серого в соответствующем пикселе, где 0 - полностью черный пиксель, а 1 - абсолютно белый пиксель.

На время отложив второй, скрытый слой, я вкратце расскажу про последний, выходной слой. В нашем случае там всего десять нейронов. Это числа от 0 до 9.
Активация в них выражает уверенность нейросети в том, что ответ - именно этот нейрон. 

А теперь, вернемся с скрытому слою. Зачастую он состоит из нескольких слоев нейронов внутри себя. Скрытый слой выполняет саму функцию, задачу нейросети.
В нашем случае, в скрытом слое есть два слоя. Первый слой определяет маленькие грани, небольшие части рисунка, деля его. 
После первый слой передает сигнал второму, который определяет уже более конкретные фигуры. 
И в итоге второй скрытый слой передает окончательную информацию выходному слою, дабы он выбрал, какая из цифр лучше всего подходит по заданным данным, и наконец то вернул значение пользователю.

------------------------------------------------------------------------------------------------------------------------------------------------------

Но как же это работает на подкорке?
Тут запрятана сложная математика. 

У каждого соединения нейрона с нейроном есть вес. Этот вес - так же просто число, которое используется для масштабирования входных данных. При умножении входных значений на соответствующие веса определяется, как сильно каждый вход влияет на выход нейрона. 
Затем возьмем все активации первого слоя и посчитаем их взвешенную сумму, согласно этим весам, получив формулу, где w - вес, а a - активация:

w1a1 + w2a2 + w3a3 + w4a4 + ... + wnan

Если недостаточно "активированные" нейроны мы сведем к 0, то получим область только из нужных нейронов. А если надо определить именно есть ли там грань, то стоит добавить несколько обратных активаций, которые четче обрисуют нужную грань.

Когда мы вычислим взвешенную сумму, у нас может получиться любое число. Но, как я говорил ранее, активация - число от 0 до 1. Поэтому нам нужна функция, которая сведет полученный диапазон значений к нужному. 
Функция, которая выполняет эту задачу, называется Сигмоидой (σ), реже - логистической кривой. 
Она выглядит так:

σ(x) = 1/(1+e^-x)

Благодаря Сигмоиде сильно отрицательный ввод становится близким к нулю, а сильно положительный - к единице, и она монотонно возрастает.

Таким образом, активация нейрона - мера того, насколько положительная взвешенная сумма в формуле σ(w1a1 + w2a2 + w3a3 + w4a4 + ... + wnan)

Если я вдруг захочу, чтоб активация нейрона была при значении, большем 10, то мне нужно добавить в функцию это условие до передачи её в Сигмоиду:

σ(w1a1 + w2a2 + w3a3 + w4a4 + ... + wnan - 10)    -> Такое число называется "сдвиг" (Bias)

Сдвиг определяет, насколько большой должна быть взвешенная сумма, чтобы нейрон стал достаточно активным. 
Эта запись лишь схематична. В реалиях масштабов нейросетей она выглядит так:

a0^(1) = σ(w0.0 * a0^(0) + w0.1 * a1^(0) + ... + w0.n * an^(0) +b0)  
Где в a1^(0) единица обозначает порядок нейрона, а степень - слой.
А в w0.1 0 - слой, а 1 - порядок веса.
b0 - сдиг

Такая запись выглядит достаточно громоздко. Чаще ее записывают иначе, получая результат матричным произведением. 

    σ (  [ w0.0   w0.1   ...   w0.n ] [ a0^(0) ]    [  b0  ]  )
      |  | w1.0   w1.1   ...   w1.n | | a1^(0) |  + |  b1  |  |
      |  | ...    ...    ...    ... | |  ...   |    |  ... |  |
      (  [ wk.0   wk.1   ...   wk.n ] [ an^(0) ]    [  bn  ]  )

Веса - в матрицу
Активации - в вектор - столбец
Складывая с ними веса в вектор - столбец
Оборачивая всю запись в Сигмоиду
Мы получаем итоговый результат

------------------------------------------------------------------------------------------------------------------------------------------------------

Помните, я говорил вначале, что нейрон - просто ячейка? Правильнее будет сказать, что нейрон - это функция, которая на выходе возвращает число: активацию. 

Итак, сейчас мы разобрали всего 1 нейрон. Каждый нейрон соединен со всеми нейронами последующего слоя. А теперь представьте, насколько громоздкой выглядит конструкция, где 784 входных нейрона соединены с определенным количеством, к примеру 16 нейронами 1 скрытого слоя и еще 16 нейронами 2 скрытого слоя, выводящими свои данные к выходному слою из 10 нейронов...
Итого, наша "простая" нейросеть, которая по сути как "Hello World" в программировании, содержит около 13 тысяч весов и сдвигов. 

Попробуйте представить настройку всех этих весов и сдвигов вручную, намеренный подбор чисел, чтобы второй слой выбирал грани, третий слой выбирал шаблоны и так далее... 
Ужаснуться можно. Возможно, даже испугаться и бросить идею изучать нейросети, если не узнать последующее. 

Хвала программистам, самим нам не прийдется все эти цифры регулировать. Разумеется, для серьезных объемов данных нужны серьезные решения. И наше решение - обучающий алгоритм. 
Задача этого алгоритма заключается в том, что он должен показать сети некоторый набор данных, сожержащий изображения цифр с их обозначением и отрегулировать 13 тысяч весов и смещений, чтобы улучшить распознавание на тренировочном наборе.

После обучения нейросети мы показываем ей набор новых данных, никогда не виденных ей прежде, чтобы проверить, как точно она определит новые цифры. 

Если в нейросеть ввести абсолютно неотрегулированные значения, а после показать ей изображение, допустим, семерки, на выходе мы получим отвратительный результат:
У нас в выходном слое будет гореть сразу несколько нейронов. Эта ошибка - исключение, которое надо обработать. Для этого создадим функцию ошибки. 

Эта функция должна сообщать нейросети, что на выходе большая часть нейронов должна иметь активацию "0". А единица здесь - это бред. 
Говоря математическим языком, нужно добавить квадрат разности между каждых плохим значением и его правильной величиной. 

После всех этих манипуляций рассмотрим среднее значение ошибки (кол-во ошибок к всем итерациям) на большом объеме данных. 
Среднее значение ошибки - мера того, насколько плохой является наша нейросеть. 

Основная задача обучения нейросети - минимизация значения ошибки. Как только значение достигнет нужных значений, нейросеть можно считать готовой к использованию. 

------------------------------------------------------------------------------------------------------------------------------------------------------

Это все звучит и выглядит достаточно замудрено, и так оно и есть. Если за нейросетями будущее, то простыми они явно быть не могут. Какова цена, таков и результат.
